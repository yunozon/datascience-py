{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "# モデル\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# model_selection\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. データを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/vgsales.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. サンプルデータの表示, カラム名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータの表示\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カラム名の表示\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 欠損値の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Object typeの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 各からむの要約統計量を表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 各カラム同士の散布図とそれぞれのカラムのヒストグラムを表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "\n",
    "# ヒストグラムが表示されるが、今回は値がずれているので、表示が小さい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7。それぞれのカテゴリのレコード数を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリー数\n",
    "category_columns = df.select_dtypes(include=['object']).columns\n",
    "df[category_columns].nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 外れ値の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Rank']==16600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('Global_Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publisherごとに売上を集計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.groupby('Publisher').sum().sort_values('Global_Sales', ascending=False)[:10].plot.bar(y='Global_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('Year', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれのジャンルの数を見る\n",
    "df['Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プラットフォームを見る\n",
    "df['Platform'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.カラム名の相関"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "# df[num_columns].corr()\n",
    "sns.heatmap(df[num_columns].corr(), annot=True, cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('Global_Sales', ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/vgsales.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 欠損値の数を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['Publisher'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['Year'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publisherがnanのデータのindexを取得\n",
    "pub_na_idx = df[df['Publisher'].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[[]]とすることでデータフレームのまま代入できる\n",
    "# fillnaでnanを埋める\n",
    "df[['Publisher']] = df[['Publisher']].fillna('NaN').iloc[pub_na_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[pub_na_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# もう一回読み込む\n",
    "df = pd.read_csv(\"../data/vgsales.csv\")\n",
    "year_na_idx = df[df['Year'].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publisherの欠損値をNaN, Yearの欠損値をYearの中央値で埋める\n",
    "df.fillna({'Publisher': 'NaN', 'Year': df['Year'].median()}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[year_na_idx][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknowも欠損値\n",
    "# Unknowを他の欠損値と同じように扱うかを考える\n",
    "pub_nan_df = df[df['Publisher'] == 'NaN']\n",
    "pub_unknown_df = df[df['Publisher'] == 'Unknown']\n",
    "pub_missing_df = pd.concat([pub_nan_df, pub_unknown_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覚的にUnknownとNaNの分布の違いを見る\n",
    "sns.pairplot(pub_missing_df, hue='Publisher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値を扱うためのライブラリ\n",
    "from sklearn.impute import SimpleImputer\n",
    "df = pd.read_csv(\"../data/vgsales.csv\")\n",
    "# あくまで学習データのみで行う\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df['Year'] = imputer.fit_transform(df[['Year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[year_na_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publisherも同様にする\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "df['Publisher'] = imputer.fit_transform(df[['Publisher']])[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[pub_na_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PlatformごとにYearの中央値を計算して埋める\n",
    "# platform_year_dict = df.groupby('Platform').median()['Year'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# データ読み込み\n",
    "df = pd.read_csv(\"../data/vgsales.csv\")\n",
    "\n",
    "# Year列のデータ型を確認し、必要に応じて数値型に変換\n",
    "if df['Year'].dtype != 'float64' and df['Year'].dtype != 'int64':\n",
    "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "\n",
    "# 欠損値の処理（NaNを除外して中央値を計算）\n",
    "platform_year_dict = df.groupby('Platform')['Year'].median().to_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df.apply(\n",
    "    lambda row: platform_year_dict[row['Platform']] if np.isnan(row['Year']) and row['Platform'] in platform_year_dict else row['Year'], \n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[year_na_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠損値対応"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 'Year'の欠損値をKNNで補完する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回は目的変数はなしとして,knnで欠損値を埋める\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# 特徴量スケーリング\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# データ分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/vgsales.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publihserの欠損値を埋める\n",
    "df[['Publisher']] = df[['Publisher']].fillna('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/vgsales.csv\")\n",
    "y_nan_col = 'Year'\n",
    "\n",
    "# get_dummiesでダミー変数するとnameカテゴリのせいで、カラムが大量に増える\n",
    "# nameカテゴリがあるが、nameカテゴリは予測に関係ないので、削除する, 過学習を防ぐためにも削除する\n",
    "df.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "X = df.drop(y_nan_col, axis=1)\n",
    "y = df[y_nan_col]\n",
    "\n",
    "\n",
    "# 数値カラムだけを抽出して、標準化する(get_dummiesでダミー変数をした後に標準化はしない)\n",
    "num_columns = X.select_dtypes(include=np.number).columns.to_list()\n",
    "\n",
    "# dummy変数化\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 数値カラムだけ標準化\n",
    "scaler = StandardScaler()\n",
    "X[num_columns] = scaler.fit_transform(X[num_columns])\n",
    "\n",
    "# データ分割\n",
    "# df['Year']が欠損値のデータのインデックスを取得\n",
    "test_indexes = df[df['Year'].isna()].index\n",
    "# df['Year']が欠損値ではないデータのインデックスを取得\n",
    "train_indexes = df[~df['Year'].isna()].index\n",
    "\n",
    "X_train, X_test = X.iloc[train_indexes], X.iloc[test_indexes]\n",
    "y_train, y_test = y.iloc[train_indexes], y.iloc[test_indexes]\n",
    "\n",
    "# モデルの学習\n",
    "knn = KNeighborsRegressor(n_neighbors=3).fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このyearに使われたデータを取得する\n",
    "# 今回はindexが210, 239, 278のデータが使われた\n",
    "knn.kneighbors(X_test.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[210, 239, 278]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dummiesでダミー変数するとnameカテゴリのせいで、カラムが大量に増える\n",
    "# nameカテゴリがあるが、nameカテゴリは予測に関係ないので、削除する, 過学習を防ぐためにも削除する\n",
    "\n",
    "# 数値カラムだけを抽出して、標準化する(get_dummiesでダミー変数をした後に標準化はしない)\n",
    "pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. KNNimputerを使って欠損値を埋める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# データフレームで出力するようにする\n",
    "imputer.set_output(transform='pandas')\n",
    "\n",
    "# ダミー変数\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "df[num_columns] = scaler.fit_transform(df[num_columns])\n",
    "\n",
    "# すべての欠損値を埋める\n",
    "# fit_transformがあるものは、set_outputでデータフレームで出力するようにできる\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.iloc[test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠損値代入の比較\n",
    "\n",
    "ペンギンデータセット('penguins_size.csv')を使用して、以下の異なる欠損値対応方法による精度を比較します。\n",
    "\n",
    "#### 比較するケース\n",
    "\n",
    "1. **欠損値を落としたケース**\n",
    "   - `.dropna()`\n",
    "2. **欠損値を新しいカテゴリとしたケース**\n",
    "   - 数値カラムは中央値で代入\n",
    "3. **`sklearn.impute.SimpleImputer()`を使用したケース**\n",
    "   - `.fit_transform()`\n",
    "4. **欠損値をkNNで予測したケース**\n",
    "   - カテゴリカルカラムは最頻値で代入\n",
    "   - `sklearn.impute.KNNImputer(n_neighbors).fit_transform()`\n",
    "\n",
    "#### モデル\n",
    "\n",
    "- 任意のモデルを使用可能（回答例ではロジスティック回帰を使用）\n",
    "\n",
    "#### 評価方法\n",
    "\n",
    "- 5-fold cross-validationを3回繰り返す\n",
    "- `Pipeline`や`ColumnTransformer`クラスを使用\n",
    "- 評価指標はloglossを使用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penguins Size Dataset (`penguins_size.csv`)\n",
    "\n",
    "このデータセットはペンギンのサイズと種別に関する情報を記録しています。\n",
    "\n",
    "#### データセットの内容\n",
    "\n",
    "- `species`: ペンギンの種別（Chinstrap, Adelie, Gentoo）\n",
    "- `culmen_length_mm`: くちばしの長さ (mm)\n",
    "- `culmen_depth_mm`: くちばしの高さ (mm)\n",
    "- `flipper_length_mm`: 翼の長さ (mm)\n",
    "- `body_mass_g`: 体重 (g)\n",
    "- `island`: 調査した島（Dream, Torgersen, Biscoe） @南極大陸\n",
    "- `sex`: 性別\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "df = pd.read_csv(\"../data/penguins_size.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.pairplot(df, hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seabornで相関係数を見る\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損値をdorpnaで削除するケース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['sex'] == '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sexからむで.となっているデータをNanとする\n",
    "df.loc[df[df['sex']=='.'].index, 'sex'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df = pd.read_csv(\"../data/penguins_size.csv\")\n",
    "# sexからむで.となっているデータをNanとする\n",
    "df.loc[df[df['sex']=='.'].index, 'sex'] = np.nan\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "target = 'species'\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# ダミー変数の生成(columntransofomerで作成)\n",
    "# 標準化(columntransofomerで作成)\n",
    "\n",
    "# columntransofomerでダミー変数と標準化を同時に行う\n",
    "# 数値カラムとカテゴリカルカラムを分ける\n",
    "num_columns = X.select_dtypes(include=np.number).columns.to_list()\n",
    "cat_columns = X.select_dtypes(exclude=np.number).columns.to_list()\n",
    "\n",
    "# ColumnTransformerの作成\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_columns),\n",
    "        ('cat', OneHotEncoder(drop='first'), cat_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# パイプラインの作成\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# クロスバイリデーションの設定\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=0)\n",
    "\n",
    "# モデルの評価\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_log_loss')\n",
    "\n",
    "# 平均値を出力\n",
    "print(f'平均値: {-np.mean(scores)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を格納するでディクショナリー\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/penguins_size.csv\")\n",
    "df.loc[df[df['sex']=='.'].index, 'sex'] = np.nan\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "target = 'species'\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# ダミー変数\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# CV\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=0)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_log_loss')\n",
    "results['drop'] = -np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損値を新カテゴリーとする and 数値カラムは中央値で代入\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/penguins_size.csv\")\n",
    "df.loc[df[df['sex']=='.'].index, 'sex'] = np.nan\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "target = 'species'\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=0)\n",
    "# ダミー変数生成クラスを自作(pipelineに入れるため)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class GetDummies(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.columns = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = pd.get_dummies(X).columns\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_new = pd.get_dummies(X)\n",
    "        return X_new.reindex(columns=self.columns, fill_value=0)\n",
    "\n",
    "# 欠損値を代入処理はcross_val_scoreの中で行う\n",
    "# pipelineはデータフレーム全体の処理\n",
    "# 数値カラムだけの処理pipelineでできない→ColumnTransformerで行う\n",
    "num_columns = X.select_dtypes(include=np.number).columns.to_list()\n",
    "cat_columns = X.select_dtypes(exclude=np.number).columns.to_list()\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('imputer_num', SimpleImputer(strategy='median', fill_value='Nan'), num_columns),\n",
    "        ('imputer_cat', SimpleImputer(strategy='constant', fill_value='Nan'), cat_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "ct.set_output(transform='pandas')\n",
    "\n",
    "# pipeline(dummies + scaele + model)\n",
    "# pipelineはfitとtransformeするものしか入らない\n",
    "# get_dummies自分でクラスを作る必要がある\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('ct', ct),\n",
    "    ('dummies', GetDummies()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_log_loss')\n",
    "results['median'] = -np.mean(scores)\n",
    "results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class GetDummies(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.columns = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = pd.get_dummies(X).columns\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_new = pd.get_dummies(X)\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GetDummies()\n",
    "gb.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠損値をKNNで予測したケース(カテゴリカルカラムは最頻値を使用)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/penguins_size.csv\")\n",
    "df.loc[df[df['sex']=='.'].index, 'sex'] = np.nan\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "target = 'species'\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=0)\n",
    "# ダミー変数生成クラスを自作(pipelineに入れるため)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class GetDummies(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.columns = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = pd.get_dummies(X).columns\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_new = pd.get_dummies(X)\n",
    "        return X_new.reindex(columns=self.columns, fill_value=0)\n",
    "\n",
    "# 欠損値を代入処理はcross_val_scoreの中で行う\n",
    "# pipelineはデータフレーム全体の処理\n",
    "# 数値カラムだけの処理pipelineでできない→ColumnTransformerで行う\n",
    "num_columns = X.select_dtypes(include=np.number).columns.to_list()\n",
    "cat_columns = X.select_dtypes(exclude=np.number).columns.to_list()\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "# KNNimputerの前に標準化する\n",
    "# 数値カラム用のパイプライン\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('imputer_num', KNNImputer())\n",
    "])\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('imputer_cat', SimpleImputer(strategy='most_frequent'), cat_columns),\n",
    "        ('num_pipeline', num_pipeline, num_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "ct.set_output(transform='pandas')\n",
    "\n",
    "# pipeline(dummies + scaele + model)\n",
    "# pipelineはfitとtransformeするものしか入らない\n",
    "# get_dummies自分でクラスを作る必要がある\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('ct', ct),\n",
    "    ('dummies', GetDummies()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_log_loss')\n",
    "results['knn'] = -np.mean(scores)\n",
    "results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
